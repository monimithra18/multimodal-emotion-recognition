# Multi-Modal Emotion Recognition Using Image, Audio, and Text Analysis #

## Overview

This project focuses on developing a real-time emotion recognition system that integrates text, image, and audio analysis. Traditional emotion recognition models often rely on a single data modality, which limits their ability to capture the full complexity of human emotions. Our multi-modal approach provides a more comprehensive and accurate understanding of emotions by combining these three types of data inputs.


## Problem Statement

Existing unimodal emotion recognition systems struggle to recognize emotions effectively, especially when analyzing just one modality. There is a need for a system capable of processing diverse data inputs to enhance emotion detection across different contexts, such as healthcare, education, and entertainment.

## Project Highlights 

*Dataset Integration*: Merged datasets from different modalities:

Text: bitcointweets.csv for sentiment analysis from tweets.
Audio: RAVDESS and TESS datasets for emotional speech recognition.
Image: emotion-detection-fer dataset for facial expression analysis.

*Models and Techniques Used:*
CNNs for image-based emotion detection.
MFCC for feature extraction from audio signals.
LSTM networks for sequential data processing in text and audio.
Advanced text processing with tokenization, lemmatization, and stop word removal.

*Real-Time System:*
Designed for low-latency emotion recognition across all three modalities.

_Performance Metrics_
Text Model Accuracy: 96%
Audio Model Accuracy: 89%
Image Model Accuracy: 91%
Cross-modal analysis highlights the benefits of combining multiple methods for more accurate emotion prediction.

## Future Work
- Cross-modal integration enhancements
- Support for multiple languages
- Edge computing implementation
- Collaboration with psychology and cognitive science for model improvement


### Feel free to contribute or reach out with any questions!
