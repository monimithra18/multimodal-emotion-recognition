{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5919dbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter('ignore')\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "from matplotlib.pyplot import specgram\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "46d549b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing  C:\\Users\\iamsa\\Downloads\\Speech_emotion_recognition-main\\Speech_emotion_recognition-main\\angry.wav\n"
     ]
    }
   ],
   "source": [
    "header = 'chroma_stft rms spectral_centroid spectral_bandwidth rolloff zero_crossing_rate'\n",
    "for i in range(1, 21):\n",
    "    header += f' mfcc{i}'\n",
    "header += ' label'\n",
    "header = header.split()\n",
    "\n",
    "def process_audio(taalfile):\n",
    "    \n",
    "    print('Processing ',taalfile)\n",
    "    y, sr = librosa.load(taalfile, mono=True, duration=30)\n",
    "    rms = librosa.feature.rms(y=y)\n",
    "    chroma_stft = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "    spec_cent = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "    spec_bw = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n",
    "    rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
    "    zcr = librosa.feature.zero_crossing_rate(y)\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr)\n",
    "    to_append = f' {np.mean(chroma_stft)} {np.mean(rms)} {np.mean(spec_cent)} {np.mean(spec_bw)} {np.mean(rolloff)} {np.mean(zcr)} '    \n",
    "    for e in mfcc:\n",
    "        to_append += f' {np.mean(e)}'\n",
    "        \n",
    "    return to_append\n",
    "\n",
    "live_data = process_audio(r\"C:\\Users\\iamsa\\Downloads\\Speech_emotion_recognition-main\\Speech_emotion_recognition-main\\angry.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d6340dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "live_data_list = []\n",
    "for i in live_data.split():\n",
    "    live_data_list.append(float(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "47c1e236",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "\n",
    "# Define the sequential model\n",
    "audio_model = Sequential()\n",
    "\n",
    "# Adding LSTM layers for audio analysis\n",
    "audio_model.add(LSTM(128, input_shape=(26, 1), return_sequences=True))\n",
    "audio_model.add(Dropout(0.2))\n",
    "audio_model.add(LSTM(64))\n",
    "audio_model.add(Dropout(0.2))\n",
    "\n",
    "# Adding Dense layers with BatchNormalization and Dropout\n",
    "audio_model.add(Dense(256, activation='relu'))\n",
    "audio_model.add(BatchNormalization())\n",
    "audio_model.add(Dropout(0.4))\n",
    "audio_model.add(Dense(128, activation='relu'))\n",
    "audio_model.add(BatchNormalization())\n",
    "audio_model.add(Dropout(0.4))\n",
    "\n",
    "# Final output layer\n",
    "audio_model.add(Dense(14, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a63c8186",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the weights\n",
    "audio_model.load_weights(r\"model_weights.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ace89dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming live_data_list is a NumPy array\n",
    "live_data_list = np.array(live_data_list)\n",
    "\n",
    "# Reshape the input to match the expected shape (batch_size, sequence_length, input_features)\n",
    "live_data_list = live_data_list.reshape((1, live_data_list.shape[0], 1))\n",
    "\n",
    "# Make predictions\n",
    "rs = audio_model.predict(live_data_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7a6dfb57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: male_angry\n"
     ]
    }
   ],
   "source": [
    "outputs = ['male_neutral', 'male_sad', 'male_fear', 'male_happy', 'male_disgust', 'male_angry', 'male_surprise', 'female_surprise', 'female_neutral', 'female_disgust', 'female_fear', 'female_sad', 'female_happy', 'female_angry']\n",
    "\n",
    "d = dict(zip(outputs, range(0, len(outputs))))\n",
    "\n",
    "def return_key(val):\n",
    "    for key, value in d.items():\n",
    "        if value == val:\n",
    "            return key\n",
    "    return 'Key Not Found'\n",
    "\n",
    "\n",
    "# Get the index with the highest probability\n",
    "predicted_index = np.argmax(rs)\n",
    "\n",
    "# Map the index to the corresponding class label\n",
    "predicted_class = return_key(predicted_index)\n",
    "\n",
    "print(\"Predicted Class:\", predicted_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cfdd91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38de1ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
